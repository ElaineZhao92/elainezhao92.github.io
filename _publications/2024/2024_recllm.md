---
title:          "Fine-tuning Large Language Model based Explainable Recommendation with Explainable Quality Reward"
date:           2024-03-24
selected:       true
type:           publication
tags:           ["# recommender system", "# LLM fine-tuning", "# explainability"]
pub:            "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2024"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
#pub_date:       "2024"
# semantic_scholar_id: 204e3073870fae3d05bcbc2f6a8e263d9b72e776  # use this to retrieve citation count
abstract: >-
  Large language model-based explainable recommendation (LLM-based ER) systems can provide remarkable human-like explanations and have widely received attention from researchers. However, the original LLM-based ER systems face three low-quality problems in their generated explanations, i.e., lack of personalization, inconsistency, and questionable explanation data. To address these problems, we propose a novel LLM-based ER model denoted as LLM2ER to serve as a backbone and devise two innovative explainable quality reward models for fine-tuning such a backbone in a reinforcement learning paradigm, ultimately yielding a fine-tuned model denoted as LLM2ER-EQR, which can provide high-quality explanations. LLM2ER-EQR can generate personalized, informative, and consistent high-quality explanations learned from questionable-quality explanation datasets. Extensive experiments conducted on three real-world datasets demonstrate that our model can generate fluent, diverse, informative, and highly personalized explanations.
cover:          /images/LLM-EQR.png
authors:
  - Mengyuan Yang
  - Mengying Zhu#
  - Yan Wang
  - Linxun Chen
  - Yilei Zhao
  - Xiuyuan Wang
  - Bing Han
  - Xiaolin Zheng
  - Jianwei Yin
links:
  Paper: https://ojs.aaai.org/index.php/AAAI/article/view/28777
---
